# exploringActivations

## Synopsis
The goal of this project is to experiment Selu, Gelu and swish activation functions and compare it with older activation functions such as RELU and Sigmoid. 

## Getting Started
This section describes the preqrequisites, and contains instructions, to get the project up and running.

### Setup
 This project can be easily set up by installing following packages and versions : 
 Python 3.7.1\
 Tensorflow 1.11.1
 
 
### Usage
 To run the experiments and analyze our code: 
 
 Go to notebooks folder and run the 'Tutorial' jupyter notebook

### Built With
* [Python](https://www.python.org/)
* [Tensorflow.keras](https://keras.io/)
* [Scikit-learn](https://scikit-learn.org/stable/) 

## Contributing Guidelines
There are no specific guidelines for contributing, apart from a few general guidelines we tried to follow, such as:
* Code should follow PEP8 standards as closely as possible

If you see something that could be improved, send a pull request! 
We are always happy to look at improvements and new experiments, to ensure that `einstein`, as a project, is the best version of itself. 

If you think something should be done differently (or is just-plain-broken), please create an issue.

## Contributors
See the [Contributors]() file for details about individual contributions.
